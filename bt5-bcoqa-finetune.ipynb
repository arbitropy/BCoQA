{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2023-11-04T12:44:08.846686Z","iopub.status.busy":"2023-11-04T12:44:08.846341Z","iopub.status.idle":"2023-11-04T12:44:21.276256Z","shell.execute_reply":"2023-11-04T12:44:21.275184Z","shell.execute_reply.started":"2023-11-04T12:44:08.846656Z"},"id":"MOsHUjgdIrIW","outputId":"f84a093e-147f-470e-aad9-80fb51193c8e","trusted":true},"outputs":[],"source":["# Import the necessary libraries\n","from datasets import load_dataset, Dataset\n","from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n","from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from normalizer import normalize\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","# check for cuda\n","print(torch.cuda.is_available())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270,"referenced_widgets":["69caab03d6264fef9fc5649bffff5e20","3f74532faa86412293d90d3952f38c4a","50615aa59c7247c4804ca5cbc7945bd7","fe962391292a413ca55dc932c4279fa7","299f4b4c07654e53a25f8192bd1d7bbd","ad04ed1038154081bbb0c1444784dcc2","7c667ad22b5740d5a6319f1b1e3a8097","46c2b043c0f84806978784a45a4e203b","80e2943be35f46eeb24c8ab13faa6578","de5956b5008d4fdba807bae57509c393","931db1f7a42f4b46b7ff8c2e1262b994","6c1db72efff5476e842c1386fadbbdba","ccd2f37647c547abb4c719b75a26f2de","d30a66df5c0145e79693e09789d96b81","5fa26fc336274073abbd1d550542ee33","2b34de08115d49d285def9269a53f484","d426be871b424affb455aeb7db5e822e","160bf88485f44f5cb6eaeecba5e0901f","745c0d47d672477b9bb0dae77b926364","d22ab78269cd4ccfbcf70c707057c31b","d298eb19eeff453cba51c2804629d3f4","a7204ade36314c86907c562e0a2158b8","e35d42b2d352498ca3fc8530393786b2","75103f83538d44abada79b51a1cec09e","f6253931d90543e9b5fd0bb2d615f73a","051aa783ff9e47e28d1f9584043815f5","0984b2a14115454bbb009df71c1cf36f","8ab9dfce29854049912178941ef1b289","c9de740e007141958545e269372780a4","cbea68b25d6d4ba09b2ce0f27b1726d5","5781fc45cf8d486cb06ed68853b2c644","d2a92143a08a4951b55bab9bc0a6d0d3","a14c3e40e5254d61ba146f6ec88eae25","c4ffe6f624ce4e978a0d9b864544941a","1aca01c1d8c940dfadd3e7144bb35718","9fbbaae50e6743f2aa19342152398186","fea27ca6c9504fc896181bc1ff5730e5","940d00556cb849b3a689d56e274041c2","5cdf9ed939fb42d4bf77301c80b8afca","94b39ccfef0b4b08bf2fb61bb0a657c1","9a55087c85b74ea08b3e952ac1d73cbe","2361ab124daf47cc885ff61f2899b2af","1a65887eb37747ddb75dc4a40f7285f2","3c946e2260704e6c98593136bd32d921","50d325cdb9844f62a9ecc98e768cb5af","aa781f0cfe454e9da5b53b93e9baabd8","6bb68d3887ef43809eb23feb467f9723","7e29a8b952cf4f4ea42833c8bf55342f","dd5997d01d8947e4b1c211433969b89b","2ace4dc78e2f4f1492a181bcd63304e7","bbee008c2791443d8610371d1f16b62b","31b1c8a2e3334b72b45b083688c1a20c","7fb7c36adc624f7dbbcb4a831c1e4f63","0b7c8f1939074794b3d9221244b1344d","a71908883b064e1fbdddb547a8c41743","2f5223f26c8541fc87e91d2205c39995"]},"execution":{"iopub.execute_input":"2023-11-04T12:44:28.686463Z","iopub.status.busy":"2023-11-04T12:44:28.685579Z","iopub.status.idle":"2023-11-04T12:44:52.561250Z","shell.execute_reply":"2023-11-04T12:44:52.560379Z","shell.execute_reply.started":"2023-11-04T12:44:28.686431Z"},"id":"s_AY1ATSIrIq","outputId":"fd0578d1-8895-443d-b56f-5908de9f1b6b","trusted":true},"outputs":[],"source":["dataset_coqa = load_dataset(\"arbitropy/bcoqa\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["prefix = \"Continue conversation:\\n\"\n","\n","def create_history_format(example, history, turnCount):\n","    \"\"\"\n","    Creates the formatted history for the conversation prompt.\n","\n","    Args:\n","        example (dict): The example containing the context, questions, and answers.\n","        history (list): The list of previous questions and answers in the conversation.\n","        turnCount (int): The number of previous turns to include in the history. If 0, include all turns.\n","\n","    Returns:\n","        str: The formatted conversation prompt.\n","\n","    \"\"\"\n","    if len(history) == 0:\n","        prompt = prefix + \"\"\"Context:\\n%s\\n\"\"\"%(example['context'])\n","    elif len(history) < turnCount or turnCount == 0:\n","        prompt = prefix +\"\"\"Context:\\n%s\\nHistory:\\n\"\"\"%(example['context'])\n","        for i in range(len(history)): # add all history\n","            prompt += \"\"\"+%s\\n-%s\\n\"\"\"%(history[i]['question'], history[i]['answer'])\n","    else:\n","        prompt = prefix +\"\"\"Context:\\n%s\\nHistory:\\n\"\"\"%(example['context'])\n","        for i in range(len(history)-turnCount,len(history)): # only the last needed turn count\n","            prompt += \"\"\"+%s\\n-%s\\n\"\"\"%(history[i]['question'], history[i]['answer'])\n","    prompt += \"\"\"+%s\\n-\"\"\"%(example['question']) # add the query question\n","    return prompt\n","\n","def denest_element(example):\n","    \"\"\"\n","    Denests an conversation by creating multiple items with different questions and answers.\n","\n","    Args:\n","        example (dict): A whole conversation item.\n","\n","    Returns:\n","        list: The list of denested items.\n","\n","    \"\"\"\n","    dict_list = []\n","    for i in range(len(example['questions'])):\n","        raw_denest_dict = {'id': example['id'], 'context': example['story'], 'question': example['questions'][i]['question'], 'answer': example['answers'][i]['answer']}\n","        # the story is modified with final prompt\n","        context = create_history_format(raw_denest_dict, dict_list, 0) # turn count is zero for entire history\n","        # add the modified one to the train_dataset, context holds the prompt\n","        dict_list.append({'id': example['id'], 'context': context, 'question': example['questions'][i]['question'], 'answer': example['answers'][i]['answer']})\n","    return dict_list\n","\n","def denest_dataset_with_context(dataset):\n","    \"\"\"\n","    Denests a dataset by denesting each conversation in the dataset into items with single question and answer.\n","\n","    Args:\n","        dataset (dataset): Whole dataset.\n","\n","    Returns:\n","        dataset: The denested dataset.\n","\n","    \"\"\"\n","    denested_list = []\n","    for i, input in enumerate(dataset):\n","        response = denest_element(input)\n","        for item in response:\n","            denested_list.append(item)\n","    return Dataset.from_list(denested_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_dataset_coqa = denest_dataset_with_context(dataset_coqa['train'])\n","valid_dataset_coqa = denest_dataset_with_context(dataset_coqa['validation'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T12:44:52.570690Z","iopub.status.busy":"2023-11-04T12:44:52.570434Z","iopub.status.idle":"2023-11-04T12:44:56.847682Z","shell.execute_reply":"2023-11-04T12:44:56.846887Z","shell.execute_reply.started":"2023-11-04T12:44:52.570667Z"},"id":"eXNLu_-nIrJI","trusted":true},"outputs":[],"source":["model_checkpoint = \"csebuetnlp/banglat5\"\n","tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n","model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n","data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def tokenize_label(examples):\n","    \"\"\"\n","    Tokenizes the input examples and prepares the model inputs for training.\n","\n","    Args:\n","        examples (dict): A dictionary containing the input examples.\n","\n","    Returns:\n","        dict: A dictionary containing the tokenized model inputs with labels.\n","\n","    \"\"\"\n","    inputs = examples['context']\n","    model_inputs = tokenizer(normalize(inputs), max_length=1024, truncation=True) # normalize library is used before tokeinzation for best result\n","    labels = tokenizer(text_target=normalize(examples[\"answer\"]), max_length=128, truncation=True)\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Map the preprocessing function across our dataset\n","train_tokenized_coqa = train_dataset_coqa.map(tokenize_label)\n","valid_tokenized_coqa = valid_dataset_coqa.map(tokenize_label)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch_size = 8\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"bcoqa-bt5\",\n","    evaluation_strategy=\"steps\",\n","    eval_steps = 10000,\n","    save_strategy = 'steps',\n","    save_steps = 10000,\n","    optim=\"adafactor\",\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    save_total_limit=2,\n","    num_train_epochs=2,\n","    predict_with_generate=True,\n","    load_best_model_at_end=True,\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_tokenized_coqa,\n","    eval_dataset=valid_tokenized_coqa,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the model\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.save_model('bcoqa-bt5')"]}],"metadata":{"colab":{"name":"Question Answering on SQUAD","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
